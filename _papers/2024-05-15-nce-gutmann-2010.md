---
layout: post
title:  "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"
date:   2024-05-15 16:00:22 +0530
categories: noise contrastive estimation
description: Gutmann, Hyvarinen (2010)
---
{%- include mathjax.html -%}
<style>body {text-align: justify}</style>


**tl;dr** The paper introduces a new approach to estimate parameters of a statistical model. It learns by comparing between the given data samples, and samples drawn from a known (noise) distribution.

The paper can be found [here](https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf).

<h2>Introduction</h2>

In unsupervised learning, one can take a probabilistic approach. Hence, given a set of data samples, we assume that they are drawn from an unknown distribution, and it is of interest to find out this distribution, given the samples we have.


Specifically, given a set of samples $X := \\{ x_i ; i= 1,2...N \\} $, drawn from an unknown distribution $P_d(x)$. We do not know the functional form of this distribution. However, we assume a functional form of this distribution denoted by $P_m(x; \alpha)$, where $\alpha$ is the parameter of this distribution. For example, for a Gaussian distribution, $\alpha := \\{\mu, \sigma \\}$ where the parameters are mean $\mu$ and variance $\sigma$. 

Hence, as $\alpha$ changes, we get different distributions $P_m(x; \alpha)$. We further assume that there exists a value $\alpha^* $, for which $P_d(x) = P_m(x; \alpha^*)$ (here equality is in the sense of equality of functions, maybe?).

With this setup, we have changed the problem of finding out $P_d(x)$ to that of finding out the parameter $\alpha$ for $P_m(x; \alpha)$. 

This is the problem that the paper is looking to address: *statistical estimation of parameters of a model*.

<h2>Maximum Likelihood Estimation</h2>

Maximum Likelihood Estimation (MLE) is a common, and possibly the simplest paradigm for estimating the parameters of a statistical model. In the words of my professor, it is applied when no other assumptions on the data and priors on the parameters can be made. In simple terms, we just maximize the (log) probability that atleast the observed data is *highly likely* under the statistical model. The parameter $\alpha = \alpha_{MLE}$ that achieves this requirement is the Maximum Likelihood Estimate. 

In technical terms the objective to be *maximized* is:
\begin{equation}
J(\alpha) := log(P_m(x_1, x_2, ... x_N; \alpha)) = log \prod_{i} P_m(x_i; \alpha)
\end{equation}

Here, we make the commonly used [i.i.d](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) assumption on given data.

This estimation is not easy. Specially, note that the solution must also satisfy $\int P_m(u;\alpha) du = 1$. A closed form solution is only available for simple distributions. Even for a mixture of gaussions (Gaussian Mixture Models), we need the iterative [Expectation-Maximization](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) algorithm, that can converge to a local maxima. For complex distributions, the problem becomes intractable. 

In this (and related) work, the density constraint is dealt with using the following re-formulation:
\begin{equation}
P_m(x; \alpha) := \frac{P^0_m(x;\alpha)}{Z(\alpha)}
\end{equation}

where $Z(\alpha) := \int P_m(u;\alpha) du$ is the *partition function*. Different works maximize $P^0_m(x;\alpha)$ without any constraints, and find ways to adjust for $Z(\alpha)$. 

<h2>Noise Contrastive Estimation</h2>

The paper first takes the re-formulation above and includes the partition function as one of the parameters to be estimated. Partition function being estimated along with the parameters is a contribution in this paper. So we have:
\begin{equation}
log (P_m(x; \theta)) := log (P^0_m(x;\alpha)) + c
\end{equation}

where $\theta := \\{ \alpha, c \\}$ and $c = -log(Z(\alpha))$. Note that "integrate to 1" constraint for $P_m(x;\theta)$ will only be satisfied for a specific value of $c$.